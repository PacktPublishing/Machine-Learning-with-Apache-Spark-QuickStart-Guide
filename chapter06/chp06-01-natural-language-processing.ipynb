{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Import the required Python dependencies\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import LongType, DoubleType, IntegerType, StringType, BooleanType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import Tokenizer as NLPTokenizer\n",
    "from sparknlp.annotator import Stemmer, Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'app-20181021104936-0000'),\n",
       " ('spark.executor.memory', '2g'),\n",
       " ('spark.driver.host', '192.168.56.10'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.executor.cores', '2'),\n",
       " ('spark.jars',\n",
       "  '/opt/anaconda3/lib/python3.6/site-packages/sparknlp/lib/sparknlp.jar'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///opt/anaconda3/lib/python3.6/site-packages/sparknlp/lib/sparknlp.jar'),\n",
       " ('spark.rdd.compress', 'true'),\n",
       " ('spark.master', 'spark://192.168.56.10:7077'),\n",
       " ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'),\n",
       " ('spark.kryoserializer.buffer.max', '128m'),\n",
       " ('spark.driver.memory', '2g'),\n",
       " ('spark.driver.maxResultSize', '0'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'Natural Language Processing - Sentiment Analysis'),\n",
       " ('spark.driver.port', '34701'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.cores', '1')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (2) Instantiate a Spark Context\n",
    "conf = SparkConf().setMaster(\"spark://192.168.56.10:7077\") \\\n",
    "    .set(\"spark.jars\", '/opt/anaconda3/lib/python3.6/site-packages/sparknlp/lib/sparknlp.jar') \\\n",
    "    .setAppName(\"Natural Language Processing - Sentiment Analysis\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+----------+-----------------+----------------+-----------------+----------------------------+---------------+--------------------------+--------------+----------------------+--------+--------------------+-------------+--------------------+-----------------+-------------+-----------+--------------+--------------------+\n",
      "|  unit_id|golden|unit_state|trusted_judgments|last_judgment_at|airline_sentiment|airline_sentiment_confidence|negative_reason|negative_reason_confidence|       airline|airline_sentiment_gold|    name|negative_reason_gold|retweet_count|                text|tweet_coordinates|tweet_created|   tweet_id|tweet_location|       user_timezone|\n",
      "+---------+------+----------+-----------------+----------------+-----------------+----------------------------+---------------+--------------------------+--------------+----------------------+--------+--------------------+-------------+--------------------+-----------------+-------------+-----------+--------------+--------------------+\n",
      "|681448150| false| finalized|                3|    2/25/15 5:24|          neutral|                         1.0|           null|                      null|Virgin America|                  null| cairdin|                null|            0|@VirginAmerica Wh...|             null|2/24/15 11:35|5.70306E+17|          null|Eastern Time (US ...|\n",
      "|681448153| false| finalized|                3|    2/25/15 1:53|         positive|                      0.3486|           null|                       0.0|Virgin America|                  null|jnardino|                null|            0|@VirginAmerica pl...|             null|2/24/15 11:15|5.70301E+17|          null|Pacific Time (US ...|\n",
      "+---------+------+----------+-----------------+----------------+-----------------+----------------------------+---------------+--------------------------+--------------+----------------------+--------+--------------------+-------------+--------------------+-----------------+-------------+-----------+--------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (3) Load the labelled Airline Tweet Corpus\n",
    "schema = StructType([\n",
    "    StructField(\"unit_id\", LongType()), \n",
    "    StructField(\"golden\", BooleanType()), \n",
    "    StructField(\"unit_state\", StringType()), \n",
    "    StructField(\"trusted_judgments\", IntegerType()), \n",
    "    StructField(\"last_judgment_at\", StringType()), \n",
    "    StructField(\"airline_sentiment\", StringType()), \n",
    "    StructField(\"airline_sentiment_confidence\", DoubleType()), \n",
    "    StructField(\"negative_reason\", StringType()), \n",
    "    StructField(\"negative_reason_confidence\", DoubleType()), \n",
    "    StructField(\"airline\", StringType()), \n",
    "    StructField(\"airline_sentiment_gold\", StringType()), \n",
    "    StructField(\"name\", StringType()), \n",
    "    StructField(\"negative_reason_gold\", StringType()), \n",
    "    StructField(\"retweet_count\", IntegerType()), \n",
    "    StructField(\"text\", StringType()), \n",
    "    StructField(\"tweet_coordinates\", StringType()), \n",
    "    StructField(\"tweet_created\", StringType()), \n",
    "    StructField(\"tweet_id\", StringType()), \n",
    "    StructField(\"tweet_location\", StringType()), \n",
    "    StructField(\"user_timezone\", StringType())\n",
    "])\n",
    "\n",
    "airline_tweets_df = sqlContext.read.format('com.databricks.spark.csv').schema(schema) \\\n",
    "    .options(header = 'true', inferschema = 'false') \\\n",
    "    .load('/data/workspaces/jillur.quddus/jupyter/notebooks/Machine-Learning-with-Apache-Spark-QuickStart-Guide/chapter06/data/twitter-data/airline-tweets-labelled-corpus.csv')\n",
    "airline_tweets_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------------------------------------------------------------------------------------------------------------------+------------------------+\n",
      "|unit_id  |text                                                                                                                              |negative_sentiment_label|\n",
      "+---------+----------------------------------------------------------------------------------------------------------------------------------+------------------------+\n",
      "|681448150|@VirginAmerica What @dhepburn said.                                                                                               |false                   |\n",
      "|681448153|@VirginAmerica plus you've added commercials to the experience... tacky.                                                          |false                   |\n",
      "|681448156|@VirginAmerica I didn't today... Must mean I need to take another trip!                                                           |false                   |\n",
      "|681448158|\"@VirginAmerica it's really aggressive to blast obnoxious \"\"entertainment\"\" in your guests' faces &amp; they have little recourse\"|true                    |\n",
      "|681448159|@VirginAmerica and it's a really big bad thing about it                                                                           |true                    |\n",
      "|681448162|@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.                                          |true                    |\n",
      "|null     |null                                                                                                                              |false                   |\n",
      "|681448165|@VirginAmerica yes, nearly every time I fly VX this ���ear worm�۝ won�۪t go away :)                                               |false                   |\n",
      "|681448167|@VirginAmerica Really missed a prime opportunity for Men Without Hats parody, there. https://t.co/mWpG7grEZP                      |false                   |\n",
      "|681448169|@virginamerica Well, I didn't��_but NOW I DO! :-D                                                                                 |false                   |\n",
      "+---------+----------------------------------------------------------------------------------------------------------------------------------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (4) Since we are only interested in detecting tweets with negative sentiment, generate a new label \n",
    "# whereby if the sentiment is negative, the label is TRUE (Positive Outcome) otherwise FALSE\n",
    "airline_tweets_with_labels_df = airline_tweets_df.withColumn(\"negative_sentiment_label\", \n",
    "    when(col(\"airline_sentiment\") == \"negative\", lit(\"true\")).otherwise(lit(\"false\"))) \\\n",
    "    .select(\"unit_id\", \"text\", \"negative_sentiment_label\")\n",
    "airline_tweets_with_labels_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------------------------------------------+------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------+----------------------------------------------------------+\n",
      "|unit_id  |text                                                                    |negative_sentiment_label|tokens_1                                                                            |filtered_tokens                                                   |concatenated_filtered_tokens                              |\n",
      "+---------+------------------------------------------------------------------------+------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------+----------------------------------------------------------+\n",
      "|681448150|@VirginAmerica What @dhepburn said.                                     |false                   |[@virginamerica, what, @dhepburn, said.]                                            |[@virginamerica, @dhepburn, said.]                                |@virginamerica @dhepburn said.                            |\n",
      "|681448153|@VirginAmerica plus you've added commercials to the experience... tacky.|false                   |[@virginamerica, plus, you've, added, commercials, to, the, experience..., tacky.]  |[@virginamerica, plus, added, commercials, experience..., tacky.] |@virginamerica plus added commercials experience... tacky.|\n",
      "|681448156|@VirginAmerica I didn't today... Must mean I need to take another trip! |false                   |[@virginamerica, i, didn't, today..., must, mean, i, need, to, take, another, trip!]|[@virginamerica, today..., must, mean, need, take, another, trip!]|@virginamerica today... must mean need take another trip! |\n",
      "+---------+------------------------------------------------------------------------+------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------+----------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (5) Pre-Process the tweets using the Feature Transformers NATIVE to Spark MLlib\n",
    "# (5.1) Remove any tweets with null textual content\n",
    "# (5.2) Tokenize the textual content using the Tokenizer Feature Transformer\n",
    "# (5.3) Remove Stop Words from the sequence of tokens using the StopWordsRemover Feature Transformer\n",
    "# (5.4) Concatenate the filtered sequence of tokens into a single string for 3rd party pre-processing (-> spark-nlp)\n",
    "\n",
    "filtered_df = airline_tweets_with_labels_df.filter(\"text is not null\")\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens_1\")\n",
    "tokenized_df = tokenizer.transform(filtered_df)\n",
    "remover = StopWordsRemover(inputCol=\"tokens_1\", outputCol=\"filtered_tokens\")\n",
    "preprocessed_part_1_df = remover.transform(tokenized_df)\n",
    "preprocessed_part_1_df = preprocessed_part_1_df.withColumn(\"concatenated_filtered_tokens\", \n",
    "    concat_ws(\" \", col(\"filtered_tokens\")))\n",
    "preprocessed_part_1_df.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------------------------------------------+------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|unit_id  |text                                                                    |negative_sentiment_label|normalised_stems                                                                                                                                                                                                                                                                                                                        |\n",
      "+---------+------------------------------------------------------------------------+------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|681448150|@VirginAmerica What @dhepburn said.                                     |false                   |[[token, 1, 13, virginamerica, [sentence -> 1]], [token, 16, 23, dhepburn, [sentence -> 1]], [token, 25, 28, said, [sentence -> 1]]]                                                                                                                                                                                                    |\n",
      "|681448153|@VirginAmerica plus you've added commercials to the experience... tacky.|false                   |[[token, 1, 13, virginamerica, [sentence -> 1]], [token, 15, 18, plu, [sentence -> 1]], [token, 20, 24, ad, [sentence -> 1]], [token, 26, 36, commerci, [sentence -> 1]], [token, 38, 47, experi, [sentence -> 1]], [token, 52, 56, tacki, [sentence -> 1]]]                                                                            |\n",
      "|681448156|@VirginAmerica I didn't today... Must mean I need to take another trip! |false                   |[[token, 1, 13, virginamerica, [sentence -> 1]], [token, 15, 19, todai, [sentence -> 1]], [token, 24, 27, must, [sentence -> 1]], [token, 29, 32, mean, [sentence -> 1]], [token, 34, 37, ne, [sentence -> 1]], [token, 39, 42, take, [sentence -> 1]], [token, 44, 50, anoth, [sentence -> 1]], [token, 52, 55, trip, [sentence -> 1]]]|\n",
      "+---------+------------------------------------------------------------------------+------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('unit_id', 'bigint'),\n",
       " ('text', 'string'),\n",
       " ('negative_sentiment_label', 'string'),\n",
       " ('tokens_1', 'array<string>'),\n",
       " ('filtered_tokens', 'array<string>'),\n",
       " ('concatenated_filtered_tokens', 'string'),\n",
       " ('document',\n",
       "  'array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>>>'),\n",
       " ('tokens_2',\n",
       "  'array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>>>'),\n",
       " ('stems',\n",
       "  'array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>>>'),\n",
       " ('normalised_stems',\n",
       "  'array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>>>')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (6) Define a NLP pipeline to pre-process the tweets using the spark-nlp 3rd party library\n",
    "# (6.1) Annotate the string containing the concatenated filtered tokens using the DocumentAssembler Transformer\n",
    "# (6.2) Re-tokenize the document using the Tokenizer Annotator\n",
    "# (6.3) Apply Stemming to the Tokens using the Stemmer Annotator\n",
    "# (6.4) Clean and lowercase all the Tokens using the Normalizer Annotator\n",
    "\n",
    "document_assembler = DocumentAssembler().setInputCol(\"concatenated_filtered_tokens\")\n",
    "tokenizer = NLPTokenizer().setInputCols([\"document\"]).setOutputCol(\"tokens_2\")\n",
    "stemmer = Stemmer().setInputCols([\"tokens_2\"]).setOutputCol(\"stems\")\n",
    "normalizer = Normalizer().setInputCols([\"stems\"]).setOutputCol(\"normalised_stems\")\n",
    "pipeline = Pipeline(stages=[document_assembler, tokenizer, stemmer, normalizer])\n",
    "pipeline_model = pipeline.fit(preprocessed_part_1_df)\n",
    "preprocessed_df = pipeline_model.transform(preprocessed_part_1_df)\n",
    "preprocessed_df.select(\"unit_id\", \"text\", \"negative_sentiment_label\", \"normalised_stems\").show(3, False)\n",
    "preprocessed_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------+------------------------------------------------------------------------+-------------+\n",
      "|unit_id  |negative_sentiment_label|text                                                                    |stems        |\n",
      "+---------+------------------------+------------------------------------------------------------------------+-------------+\n",
      "|681448150|false                   |@VirginAmerica What @dhepburn said.                                     |virginamerica|\n",
      "|681448150|false                   |@VirginAmerica What @dhepburn said.                                     |dhepburn     |\n",
      "|681448150|false                   |@VirginAmerica What @dhepburn said.                                     |said         |\n",
      "|681448153|false                   |@VirginAmerica plus you've added commercials to the experience... tacky.|virginamerica|\n",
      "|681448153|false                   |@VirginAmerica plus you've added commercials to the experience... tacky.|plu          |\n",
      "|681448153|false                   |@VirginAmerica plus you've added commercials to the experience... tacky.|ad           |\n",
      "|681448153|false                   |@VirginAmerica plus you've added commercials to the experience... tacky.|commerci     |\n",
      "|681448153|false                   |@VirginAmerica plus you've added commercials to the experience... tacky.|experi       |\n",
      "|681448153|false                   |@VirginAmerica plus you've added commercials to the experience... tacky.|tacki        |\n",
      "|681448156|false                   |@VirginAmerica I didn't today... Must mean I need to take another trip! |virginamerica|\n",
      "+---------+------------------------+------------------------------------------------------------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+\n",
      "|unit_id  |tokens                                                                                                    |text                                                                                                                                            |negative_sentiment_label|\n",
      "+---------+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+\n",
      "|681448241|[virginamerica, book, flight, hawaii]                                                                     |@VirginAmerica when can I book my flight to Hawaii??                                                                                            |false                   |\n",
      "|681448551|[virginamerica, pleas, flight, sjc, choic, fly, southwest, vega]                                          |@VirginAmerica can you please have flights in  SJC ? I have no choice but to fly Southwest to Vegas _���_��_                                    |false                   |\n",
      "|681448597|[virginamerica, done, thank, quick, respons, appar, faster, sit, hold]                                    |@VirginAmerica done! Thank you for the quick response, apparently faster than sitting on hold ;)                                                |false                   |\n",
      "|681448866|[virginamerica, did, dm, u, also, add, me]                                                                |@VirginAmerica I just did, how can I DM? Do u have to also add me?                                                                              |false                   |\n",
      "|681449047|[unit, uctraveladvisor, love, respond, websit, saw, realli, long, form, busi, new, seat, bad]             |@united @UCtraveladvisor - I would have loved to respond to your website until I saw the really long form. In business the new seats are bad    |true                    |\n",
      "|681449172|[unit, someon, sever, anxieti, on, person, help, next, him]                                               |@united So what does someone with severe anxiety do when the one person who can help him isn't next to him?                                     |true                    |\n",
      "|681449288|[hei, unit, flight, iah, po, leav, late, flight, on, back, iah, earli, rather, get, po, earlier]          |Hey @united why does the flight from IAH to POS leave so Late Flight, and the one back to IAH so early? I rather get to POS earlier             |true                    |\n",
      "|681449921|[unit, unit, unabl, look, flight, reserv, passeng, name, yyz, checkin, counter, wow, smh, customerservice]|\"@united In 2015, United is \"\"unable\"\" to look up a flight reservation by passenger name at the YYZ check-in counter - WOW smh #customerservice\"|true                    |\n",
      "|681450384|[unit, resolv, im, sick, tire, wait, you, want, refund, like, speak, someon, it]                          |@united resolved and im sick and tired of waiting on you. I want my refund and I'd like to speak to someone about it.                           |true                    |\n",
      "|681450451|[unit, free, wifi, flight, delai, min, allow]                                                             |@united How about free wifi when your flights delayed? Only 20 mins allowed!                                                                    |true                    |\n",
      "+---------+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (7) We could proceed to use the 3rd party annotators available in spark-nlp to train a sentiment model, such as\n",
    "# SentimentDetector and ViveknSentimentDetector respectively\n",
    "# However in this case study, we will use the Feature Extractors native to Spark MLlib to generate feature vectors \n",
    "# to train our subsequent machine learning model. In this case, we will use MLlib's TF-IDF Feature Extractor.\n",
    "\n",
    "# (7.1) Extract the normalised stems from the spark-nlp Annotator Array Structure\n",
    "exploded_df = preprocessed_df.withColumn(\"stems\", explode(\"normalised_stems\")) \\\n",
    "    .withColumn(\"stems\", col(\"stems\").getItem(\"result\")) \\\n",
    "    .select(\"unit_id\", \"negative_sentiment_label\", \"text\", \"stems\")\n",
    "exploded_df.show(10, False)\n",
    "\n",
    "# (7.2) Group by Unit ID and aggregate then normalised stems into a sequence of tokens\n",
    "aggregated_df = exploded_df.groupBy(\"unit_id\").agg(concat_ws(\" \", collect_list(col(\"stems\"))), \n",
    "    first(\"text\"), first(\"negative_sentiment_label\")) \\\n",
    "    .toDF(\"unit_id\", \"tokens\", \"text\", \"negative_sentiment_label\") \\\n",
    "    .withColumn(\"tokens\", split(col(\"tokens\"), \" \").cast(\"array<string>\"))\n",
    "aggregated_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+---------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|unit_id  |tokens                                                                                                    |text                                                                                                                                            |negative_sentiment_label|raw_features                                                                                             |features                                                                                                                                                                                                                                                                                               |\n",
      "+---------+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+---------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|681448241|[virginamerica, book, flight, hawaii]                                                                     |@VirginAmerica when can I book my flight to Hawaii??                                                                                            |false                   |(280,[44,150,273,276],[1.0,1.0,1.0,1.0])                                                                 |(280,[44,150,273,276],[4.277828552050639,1.1650798201104602,2.9830339057963395,2.9577160978120496])                                                                                                                                                                                                    |\n",
      "|681448551|[virginamerica, pleas, flight, sjc, choic, fly, southwest, vega]                                          |@VirginAmerica can you please have flights in  SJC ? I have no choice but to fly Southwest to Vegas _���_��_                                    |false                   |(280,[26,84,95,113,150,211,245,276],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                   |(280,[26,84,95,113,150,211,245,276],[3.574877371394073,3.650863278371995,2.9039259232259123,4.220396502964764,1.1650798201104602,4.085702995160064,2.895235614033935,2.9577160978120496])                                                                                                              |\n",
      "|681448597|[virginamerica, done, thank, quick, respons, appar, faster, sit, hold]                                    |@VirginAmerica done! Thank you for the quick response, apparently faster than sitting on hold ;)                                                |false                   |(280,[2,27,150,170,189,194,235,272,276],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                           |(280,[2,27,150,170,189,194,235,272,276],[1.6387712224353805,2.4967996851676713,1.1650798201104602,2.9330234852216783,2.9215364412345473,4.211137177551967,3.272066417345992,3.6121487661913045,2.9577160978120496])                                                                                    |\n",
      "|681448866|[virginamerica, did, dm, u, also, add, me]                                                                |@VirginAmerica I just did, how can I DM? Do u have to also add me?                                                                              |false                   |(280,[72,78,86,151,207,276],[2.0,1.0,1.0,1.0,1.0,1.0])                                                   |(280,[72,78,86,151,207,276],[7.035979993984044,3.272066417345992,2.2892121889544943,2.7096232274498915,4.560596609699991,2.9577160978120496])                                                                                                                                                          |\n",
      "|681449047|[unit, uctraveladvisor, love, respond, websit, saw, realli, long, form, busi, new, seat, bad]             |@united @UCtraveladvisor - I would have loved to respond to your website until I saw the really long form. In business the new seats are bad    |true                    |(280,[0,24,25,30,31,55,86,140,141,167,197,209,232],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|(280,[0,24,25,30,31,55,86,140,141,167,197,209,232],[3.364497861804961,3.3295428467713846,2.598019408159466,3.5249264409886787,2.8248428164320765,1.1474878798444876,2.2892121889544943,3.3864767685237362,2.981685287925046,3.1874603331576115,2.480338408113599,3.574877371394073,3.0972806912407402])|\n",
      "|681449172|[unit, someon, sever, anxieti, on, person, help, next, him]                                               |@united So what does someone with severe anxiety do when the one person who can help him isn't next to him?                                     |true                    |(280,[27,33,55,89,98,176,186,213,242],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                             |(280,[27,33,55,89,98,176,186,213,242],[2.4967996851676713,3.1629292584078303,1.1474878798444876,3.7445957560347023,4.144297159426117,3.366476101817067,4.9370741809349035,3.0399541960490217,2.500124709012443])                                                                                       |\n",
      "|681449288|[hei, unit, flight, iah, po, leav, late, flight, on, back, iah, earli, rather, get, po, earlier]          |Hey @united why does the flight from IAH to POS leave so Late Flight, and the one back to IAH so early? I rather get to POS earlier             |true                    |(280,[25,47,55,61,119,128,150,157,162,242,271,279],[2.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0])    |(280,[25,47,55,61,119,128,150,157,162,242,271,279],[5.196038816318932,3.798020922708283,1.1474878798444876,2.889074165089886,2.146201257200233,5.732001777851168,3.4952394603313808,3.4997246490147282,4.152955222169231,2.500124709012443,3.9598227492710607,2.5916120635844657])                     |\n",
      "|681449921|[unit, unit, unabl, look, flight, reserv, passeng, name, yyz, checkin, counter, wow, smh, customerservice]|\"@united In 2015, United is \"\"unable\"\" to look up a flight reservation by passenger name at the YYZ check-in counter - WOW smh #customerservice\"|true                    |(280,[15,29,38,41,55,70,87,100,128,150,179,261],[1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0])       |(280,[15,29,38,41,55,70,87,100,128,150,179,261],[3.67214067681928,3.3295428467713846,3.23840513477286,4.908903303968207,2.2949757596889753,3.3905253570497362,3.408949624375795,3.344927765610864,2.866000888925584,1.1650798201104602,6.040303137505684,4.157312527538187])                           |\n",
      "+---------+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+---------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (8) Generate Term Frequency Feature Vectors by passing the sequence of tokens to the HashingTF Transformer.\n",
    "# Then fit an IDF Estimator to the Featurized Dataset to generate the IDFModel.\n",
    "# Finally pass the TF Feature Vectors to the IDFModel to scale based on frequency across the corpus\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"raw_features\", numFeatures=280)\n",
    "features_df = hashingTF.transform(aggregated_df)\n",
    "\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idf_model = idf.fit(features_df)\n",
    "scaled_features_df = idf_model.transform(features_df)\n",
    "scaled_features_df.cache()\n",
    "scaled_features_df.show(8, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (9) Index the label column using StringIndexer\n",
    "# Now a label of 1.0 = FALSE (Not Negative Sentiment) and a label of 0.0 = TRUE (Negative Sentiment)\n",
    "indexer = StringIndexer(inputCol = \"negative_sentiment_label\", outputCol = \"label\").fit(scaled_features_df)\n",
    "scaled_features_indexed_label_df = indexer.transform(scaled_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13129, 1503)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (10) Split the index-labelled Scaled Feature Vectors into Training and Test DataFrames\n",
    "train_df, test_df = scaled_features_indexed_label_df.randomSplit([0.9, 0.1], seed=12345)\n",
    "train_df.count(), test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (11) Train a Classification Tree Model on the Training DataFrame\n",
    "decision_tree = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "decision_tree_model = decision_tree.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST DATASET PREDICTIONS AGAINST ACTUAL LABEL: \n",
      "+----------+-----+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|prediction|label|text                                                                                                                                               |\n",
      "+----------+-----+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0       |0.0  |@united @UCtraveladvisor - I would have loved to respond to your website until I saw the really long form. In business the new seats are bad       |\n",
      "|1.0       |0.0  |@united resolved and im sick and tired of waiting on you. I want my refund and I'd like to speak to someone about it.                              |\n",
      "|0.0       |0.0  |@USAirways why now just announce delay of 4478 from PVD when you knew captain was already on delayed flight coming in? Frustrating                 |\n",
      "|0.0       |0.0  |@AmericanAir Thanks - I note some, but not all, flights are Cancelled Flightled - are only some aircraft/runways working out of DFW today/tomorrow?|\n",
      "|0.0       |0.0  |@united I did start a claim but 8-10 weeks is unrealistic, am I really supposed to go that long with out a car seat for my child.Ridiculous!       |\n",
      "|0.0       |1.0  |@united what's the status of flight 1008 Bogot��-Houston?                                                                                          |\n",
      "|0.0       |0.0  |@united You allow shady 3rd party services to sell your tickets and then refuse to help the customer, who's left holding the bag.                  |\n",
      "|1.0       |0.0  |@SouthwestAir yes, very much so! I was looking forward to using your airlines for future flights but never again.                                  |\n",
      "|1.0       |1.0  |���@JetBlue: Our fleet's on fleek. http://t.co/dxn58rxDkS�۝@jameskraw see.                                                                         |\n",
      "|1.0       |1.0  |@JetBlue Using JetBlue miles?                                                                                                                      |\n",
      "+----------+-----+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (12) Apply the Trained Classification Tree Model to the Test DataFrame to make predictions\n",
    "test_decision_tree_predictions_df = decision_tree_model.transform(test_df)\n",
    "print(\"TEST DATASET PREDICTIONS AGAINST ACTUAL LABEL: \")\n",
    "test_decision_tree_predictions_df.select(\"prediction\", \"label\", \"text\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1503\n",
      "DenseMatrix([[725., 209.],\n",
      "             [244., 325.]])\n"
     ]
    }
   ],
   "source": [
    "# (13) Compute the Confusion Matrix for our Decision Tree Classifier on the Test DataFrame\n",
    "predictions_and_label = test_decision_tree_predictions_df.select(\"prediction\", \"label\").rdd\n",
    "metrics = MulticlassMetrics(predictions_and_label)\n",
    "print(\"N = %g\" % test_decision_tree_predictions_df.count())\n",
    "print(metrics.confusionMatrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1503\n",
      "DenseMatrix([[725., 209.],\n",
      "             [244., 325.]])\n"
     ]
    }
   ],
   "source": [
    "# (14) For completeness let us train a Decision Tree Classifier using Feature Vectors derived from the Bag of Words algorithm\n",
    "# Note that we have already computed these Feature Vectors when applying the HashingTF Transformer in Cell #8 above\n",
    "\n",
    "# (14.1) Create Training and Test DataFrames based on the Bag of Words Feature Vectors\n",
    "bow_indexer = StringIndexer(inputCol = \"negative_sentiment_label\", outputCol = \"label\").fit(features_df)\n",
    "bow_features_indexed_label_df = bow_indexer.transform(features_df).withColumnRenamed(\"raw_features\", \"features\")\n",
    "bow_train_df, bow_test_df = bow_features_indexed_label_df.randomSplit([0.9, 0.1], seed=12345)\n",
    "\n",
    "# (14.2) Train a Decision Tree Classifier using the Bag of Words Feature Vectors\n",
    "bow_decision_tree = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "bow_decision_tree_model = bow_decision_tree.fit(bow_train_df)\n",
    "\n",
    "# (14.3) Apply the Bag of Words Decision Tree Classifier to the Test DataFrame and generate the Confusion Matrix\n",
    "bow_test_decision_tree_predictions_df = bow_decision_tree_model.transform(bow_test_df)\n",
    "bow_predictions_and_label = bow_test_decision_tree_predictions_df.select(\"prediction\", \"label\").rdd\n",
    "bow_metrics = MulticlassMetrics(bow_predictions_and_label)\n",
    "print(\"N = %g\" % bow_test_decision_tree_predictions_df.count())\n",
    "print(bow_metrics.confusionMatrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (15) Persist the trained Decision Tree Classifier to disk for later use\n",
    "bow_decision_tree_model.save('/data/workspaces/jillur.quddus/jupyter/notebooks/Machine-Learning-with-Apache-Spark-QuickStart-Guide/chapter06/models/airline-sentiment-analysis-decision-tree-classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (16) Stop the Spark Context\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
